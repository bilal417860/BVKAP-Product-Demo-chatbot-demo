{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readme\n",
    "\n",
    "This notebook contains code that allows one to scrape data from the Dastex website. Following that is some text cleaning code. Also was implemented a structured JSON parser which attemps to create structured JSON from the websites. Embeddings are created from the website content and the different retrieval methods in Langchain are tested on the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from usp.tree import sitemap_tree_for_homepage\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "logging.getLogger('langchain.retrievers.self_query.base').setLevel(logging.INFO)\n",
    "\n",
    "import platform\n",
    "if platform.processor() != 'arm':\n",
    "    from pysitemap import crawler\n",
    "    from pysitemap.parsers.lxml_parser import Parser\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Tuple\n",
    "from pydantic import BaseModel\n",
    "import lark\n",
    "\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "from usp.tree import sitemap_tree_for_homepage\n",
    "from langchain import PromptTemplate\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nest_asyncio\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator, root_validator\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "import glob\n",
    "import os \n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )\n",
    "\n",
    "def upload_from_directory(dest_bucket_name: str, directory_path: str, dest_blob_name: str):\n",
    "    GCS_CLIENT = storage.Client()\n",
    "    rel_paths = glob.glob(directory_path + '/**', recursive=True)\n",
    "    bucket = GCS_CLIENT.get_bucket(dest_bucket_name)\n",
    "    for local_file in rel_paths:\n",
    "        remote_path = f'{dest_blob_name}/{\"/\".join(local_file.split(os.sep)[1:])}'\n",
    "        if os.path.isfile(local_file):\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some global variables that are useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"dastex-genai\"\n",
    "REGION = \"eu-west1\" \n",
    "EMBED_FOLDER = \"embeddings_website\"\n",
    "JSON_FILE_NAME = \"../data/scraped_website_text.json\"\n",
    "SCRAPED_TEXT_BUCKET = \"dastex-scraped-text\"\n",
    "EMBEDDINGS_BUCKET = \"dastex-chroma-embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for getting the website URLs\n",
    "\n",
    "The get_sitemap function creates an xml sitemap from the website since this is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebTools:\n",
    "    def __init__(self, root_url='https://www.dastex.de/produktportfolio/',\n",
    "        out_file = '../data/sitemap.xml',\n",
    "        exclude_urls = [\".pdf\", \".jpg\", \".zip\"]\n",
    "    ):\n",
    "        self.root_url = root_url\n",
    "        self.out_file = out_file\n",
    "        self.exclude_urls = exclude_urls\n",
    "\n",
    "    def get_sitemap(self, websites):\n",
    "        \"\"\"\n",
    "        Scrapes websites for URLs that match the given filter.\n",
    "\n",
    "        Args:\n",
    "            websites (list): A list of website URLs to scrape.\n",
    "            filter (list): A list of keywords to filter the URLs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of URLs that match the filter.\n",
    "        \"\"\"\n",
    "        crawler(\n",
    "            self.root_url, out_file=self.out_file, exclude_urls=self.exclude_urls,\n",
    "            http_request_options={\"ssl\": False}, parser=Parser\n",
    "        )\n",
    "    \n",
    "    def get_urls(self, sitemap_path):\n",
    "        urls = []\n",
    "\n",
    "        with open(sitemap_path, 'r', encoding='utf-8') as f:\n",
    "            xml_url = BeautifulSoup(f.read())\n",
    "\n",
    "        for url in xml_url.find_all('loc'):\n",
    "            urls.append(url.get_text())\n",
    "\n",
    "        self.urls = urls\n",
    "\n",
    "        return urls\n",
    "\n",
    "    def split_urls(self):\n",
    "        split_urls = [x.replace(self.root_url, '').split('/') for x in self.urls]\n",
    "        return split_urls\n",
    "\n",
    "    def group_stats(self):\n",
    "        split_urls = self.split_urls()\n",
    "\n",
    "        df = pd.DataFrame(split_urls, columns=['group_{}'.format(i) for i in range(5)]).drop(['group_4'], axis=1)\n",
    "        df['url'] = self.urls\n",
    "        df['group_3'] = df['group_3'].replace('', pd.NA)\n",
    "        df['group_2'] = df['group_2'].replace('', pd.NA)\n",
    "        df['group_1'] = df['group_1'].replace('', pd.NA)\n",
    "\n",
    "        # First find the products that have the product name in group_3\n",
    "        df_3 = df.dropna(subset=['group_3']).copy()\n",
    "        g3 = list(df_3['group_0'].unique())\n",
    "        counts_3 = df_3.groupby('group_0').count().drop(['group_2', 'group_3', 'url'], axis=1).rename({'group_1': 'count'}, axis=1)\n",
    "        c3 = counts_3.merge(df_3[['group_0', 'group_1', 'group_2', 'group_3', 'url']], left_index=True, right_on='group_0')\n",
    "\n",
    "        # Then find the products that have the product name in group_2\n",
    "        df_2 = df[~(df['group_0'].isin(g3))]\n",
    "        df_2 = df_2.dropna(subset=['group_2'])\n",
    "        g2 = list(df_2['group_0'].unique())\n",
    "        counts_2 = df_2.groupby('group_0').count().drop(['group_2', 'group_3', 'url'], axis=1).rename({'group_1': 'count'}, axis=1)\n",
    "        c2 = counts_2.merge(df_2[['group_0', 'group_1', 'group_2', 'url']], left_index=True, right_on='group_0')\n",
    "\n",
    "        # Finally the products that have the product name in group_1\n",
    "        # It appears that these pages do not contain individual product information and instead each\n",
    "        # Page contains information about multiple products\n",
    "        df_1 = df[~(df['group_0'].isin(g2+g3))]\n",
    "        df_1 = df_1.dropna(subset=['group_1'])\n",
    "        g1 = list(df_1['group_0'].unique())\n",
    "        counts_1 = df_1.groupby('group_0').count().drop(['group_2', 'group_3', 'url'], axis=1).rename({'group_1': 'count'}, axis=1)\n",
    "        c1 = counts_1.merge(df_1[['group_0', 'group_1', 'url']], left_index=True, right_on='group_0')\n",
    "\n",
    "        return c1, c2, c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mb/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "site = WebTools()\n",
    "urls = site.get_urls('../data/sitemap.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, c3 = site.group_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desinfektionsmittel\n",
      "entsorgungssysteme-zubehoer\n",
      "klebebaender-etiketten\n",
      "mobiliar\n",
      "papier-zubehoer\n",
      "reinigung\n",
      "spendersysteme\n",
      "spezifische-produkte\n",
      "staubbindematten\n",
      "reinraumtuecher\n",
      "schuhe-socken\n",
      "zwischenbekleidung\n",
      "einweg-schutzbekleidung\n",
      "handschuhe-fingerlinge\n",
      "oberbekleidung\n"
     ]
    }
   ],
   "source": [
    "for entry in pd.concat([c1['group_0'], c2['group_0'], c3['group_0']]).unique():\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_urls = list(c1['url']) + list(c2['url']) + list(c3['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original scraping method\n",
    "\n",
    "https://bitbucket.org/niologic/frontend_pilot/src/notebooks/website_product_google_embeddings_search.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Skipping URL: {url}\")\n",
    "        continue  # Skip this URL and move to the next one\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9ßäöüAÄÖÜ ]', ' ', text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\n+', ' ', clean_text)\n",
    "\n",
    "    data = {\"text\": clean_text, \"source\": url}\n",
    "\n",
    "    combined_data.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = []\n",
    "NUM_CHARACTERS = 1000\n",
    "\n",
    "for data in combined_data:\n",
    "    text = data['text']\n",
    "    source = data['source']\n",
    "   \n",
    "    chunks = [text[i:i+NUM_CHARACTERS] for i in range(0, len(text), NUM_CHARACTERS)]\n",
    "\n",
    "    split = source.split('/')\n",
    "    index = split.index('produktportfolio')\n",
    "    groups = {}\n",
    "    for idx, x in enumerate(split[(index+1):(len(split)-1)]):\n",
    "        groups['group_' + str(idx)] = re.sub(r'[^a-zA-Z0-9ßäöüAÄÖÜ ]', ' ', x)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Create a new dictionary for each chunk with text, source, and extracted groups\n",
    "        data = {'text': chunk, 'source': source}\n",
    "        data.update(groups)\n",
    "        split_data.append(data)\n",
    "\n",
    "with open(JSON_FILE_NAME, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(split_data, file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scraping Experimentation\n",
    "\n",
    "I believe this gives cleaner text using the Langchain own `AsyncHtmlLoader()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "header_str = ' Ihre Newsletter Anmeldung Schließen Zum Hauptinhalt springen 49 7222 9696 60 info dastex com suchen Toggle navigation Produktportfolio Oberbekleidung Zwischenbekleidung Einweg Schutzbekleidung Handschuhe Fingerlinge Schuhe Socken Reinraumtücher Desinfektionsmittel Staubbindematten Reinigung Entsorgungssysteme Zubehör Spendersysteme Mobiliar Papier Zubehör Klebebänder Etiketten Spezifische Produkte Glossar Wissenswertes Overalls Kittel Jacken Hosen Hauben Textile Mundschutze Fußbekleidung Sonderlösungen Lagerware Oberbekleidung Gewebe der Oberbekleidung auf einen Blick Wissenswertes T Shirts Overalls Pullover Jacken und Hosen Lagerware Zwischenbekleidung Gewebe der Zwischenbekleidung auf einen Blick Wissenswertes Einweghauben Gesichtsschutz Schutzbrillen Einwegoveralls Einwegkittel Zubehör Einwegartikel für den Fußbereich Wissenswertes Einweghandschuhe und Fingerlinge Textile Handschuhe Wichtige Zertifizierungen und Tests für Reinraumhandschuhe Wissenswertes PU und TPE Clogs Berufsschuhe Sicherheitsschuhe Auswechselbare Einlegesohlen Reinraumsocken Wissenswertes Baumwolltücher Zellulosetücher Polyester Zellulosetücher Polyestertücher Reinraumtücher für besondere Anforderungen Sterile trockene Tücher Getränkte Tücher Alkoholgetränkte Wisch und Mopptücher Unsere Reinraumtücher auf einen Blick Wissenswertes Desinfektionsmittel auf Alkoholbasis Alkoholgetränkte Wisch und Mopptücher Desinfektionsmittel auf Basis nicht alkoholischer Wirkstoffe Biozide mit sporizider Wirkung Ergänzende Produkte Handhygiene Wissenswertes Permanent klebende Staubbindematten Abziehbare Folienstaubbindematten Bodenplatten Wissenswertes Moppsysteme Reinigungswagen und Zubehör Reinigungswerkzeuge Adhäsive Reinigungsprodukte Schwämme und Spezialtücher Reinigungstupfer Swabs Erfolgskontrolle und Training Reinigungsflüssigkeiten Wissenswertes Longopac Pactosafe Wissenswertes Spendersysteme Einwegartikel Spendersysteme Einwegüberziehschuhe Spendersysteme Desinfektionsmittel Wissenswertes Reinraumstühle und hocker Arbeitstische und Schreibtische Regalstecksysteme und Lagerlösungen Garderoben und Bänke Aufbewahrungs und Mehrzweckwagen Leitern und Tritte aus Edelstahl Wissenswertes Papier Ringbücher Blöcke Ordner Stifte Wissenswertes Klebebänder Abroller Etiketten Wissenswertes Zytostatika Schutzverpackungen Schutzunterlagen Aktuelles Downloads Produktanfrage Unternehmen Über uns ISO Zertifizierungen Forschung Entwicklung Verhaltenskodex Unternehmenspolitik Jobs und Karriere Kontakt Suche '\n",
    "\n",
    "async def my_coroutine(urls):\n",
    "    loader = AsyncHtmlLoader(urls)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "def clean_doc(document):\n",
    "    text = document.page_content\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9ßäöüAÄÖÜ ]', ' ', text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\n+', ' ', clean_text)\n",
    "    clean_text = clean_text.replace(header_str, ' ')\n",
    "    document.page_content = clean_text\n",
    "    split = document.metadata['source'].split('/')\n",
    "    index = split.index('produktportfolio')\n",
    "    groups = {}\n",
    "    for idx, x in enumerate(split[(index+1):(len(split)-1)]):\n",
    "        document.metadata['group_' + str(idx)] = re.sub(r'[^a-zA-Z0-9ßäöüAÄÖÜ ]', ' ', x)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages:   0%|          | 0/371 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 371/371 [03:57<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "docs = loop.run_until_complete(my_coroutine(urls))\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "clean_docs = [clean_doc(doc) for doc in docs_transformed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 240/240 [00:38<00:00,  6.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Scrape only the product url's\n",
    "loop = asyncio.get_event_loop()\n",
    "p_docs = loop.run_until_complete(my_coroutine(product_urls))\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "p_docs_transformed = html2text.transform_documents(p_docs)\n",
    "p_clean_docs = [clean_doc(doc) for doc in p_docs_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "PreconditionFailed",
     "evalue": "412 POST https://storage.googleapis.com/upload/storage/v1/b/dastex-scraped-text/o?uploadType=multipart&ifGenerationMatch=0: {\n  \"error\": {\n    \"code\": 412,\n    \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n    \"errors\": [\n      {\n        \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n        \"domain\": \"global\",\n        \"reason\": \"conditionNotMet\",\n        \"locationType\": \"header\",\n        \"location\": \"If-Match\"\n      }\n    ]\n  }\n}\n: ('Request failed with status code', 412, 'Expected one of', <HTTPStatus.OK: 200>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidResponse\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/cloud/storage/blob.py:2540\u001b[0m, in \u001b[0;36mBlob.upload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2540\u001b[0m     created_json \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_upload(\n\u001b[1;32m   2541\u001b[0m         client,\n\u001b[1;32m   2542\u001b[0m         file_obj,\n\u001b[1;32m   2543\u001b[0m         content_type,\n\u001b[1;32m   2544\u001b[0m         size,\n\u001b[1;32m   2545\u001b[0m         num_retries,\n\u001b[1;32m   2546\u001b[0m         predefined_acl,\n\u001b[1;32m   2547\u001b[0m         if_generation_match,\n\u001b[1;32m   2548\u001b[0m         if_generation_not_match,\n\u001b[1;32m   2549\u001b[0m         if_metageneration_match,\n\u001b[1;32m   2550\u001b[0m         if_metageneration_not_match,\n\u001b[1;32m   2551\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   2552\u001b[0m         checksum\u001b[39m=\u001b[39;49mchecksum,\n\u001b[1;32m   2553\u001b[0m         retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m   2554\u001b[0m     )\n\u001b[1;32m   2555\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_properties(created_json)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/cloud/storage/blob.py:2355\u001b[0m, in \u001b[0;36mBlob._do_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m _MAX_MULTIPART_SIZE:\n\u001b[0;32m-> 2355\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_multipart_upload(\n\u001b[1;32m   2356\u001b[0m         client,\n\u001b[1;32m   2357\u001b[0m         stream,\n\u001b[1;32m   2358\u001b[0m         content_type,\n\u001b[1;32m   2359\u001b[0m         size,\n\u001b[1;32m   2360\u001b[0m         num_retries,\n\u001b[1;32m   2361\u001b[0m         predefined_acl,\n\u001b[1;32m   2362\u001b[0m         if_generation_match,\n\u001b[1;32m   2363\u001b[0m         if_generation_not_match,\n\u001b[1;32m   2364\u001b[0m         if_metageneration_match,\n\u001b[1;32m   2365\u001b[0m         if_metageneration_not_match,\n\u001b[1;32m   2366\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   2367\u001b[0m         checksum\u001b[39m=\u001b[39;49mchecksum,\n\u001b[1;32m   2368\u001b[0m         retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m   2369\u001b[0m     )\n\u001b[1;32m   2370\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/cloud/storage/blob.py:1890\u001b[0m, in \u001b[0;36mBlob._do_multipart_upload\u001b[0;34m(self, client, stream, content_type, size, num_retries, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1886\u001b[0m upload\u001b[39m.\u001b[39m_retry_strategy \u001b[39m=\u001b[39m _api_core_retry_to_resumable_media_retry(\n\u001b[1;32m   1887\u001b[0m     retry, num_retries\n\u001b[1;32m   1888\u001b[0m )\n\u001b[0;32m-> 1890\u001b[0m response \u001b[39m=\u001b[39m upload\u001b[39m.\u001b[39;49mtransmit(\n\u001b[1;32m   1891\u001b[0m     transport, data, object_metadata, content_type, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m   1892\u001b[0m )\n\u001b[1;32m   1894\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/resumable_media/requests/upload.py:153\u001b[0m, in \u001b[0;36mMultipartUpload.transmit\u001b[0;34m(self, transport, data, metadata, content_type, timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m _request_helpers\u001b[39m.\u001b[39;49mwait_and_retry(\n\u001b[1;32m    154\u001b[0m     retriable_request, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_status_code, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_strategy\n\u001b[1;32m    155\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     response \u001b[39m=\u001b[39m func()\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/resumable_media/requests/upload.py:149\u001b[0m, in \u001b[0;36mMultipartUpload.transmit.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m result \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    146\u001b[0m     method, url, data\u001b[39m=\u001b[39mpayload, headers\u001b[39m=\u001b[39mheaders, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m    147\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_response(result)\n\u001b[1;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/resumable_media/_upload.py:114\u001b[0m, in \u001b[0;36mUploadBase._process_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m _helpers\u001b[39m.\u001b[39;49mrequire_status_code(response, (http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mOK,), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_status_code)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/resumable_media/_helpers.py:108\u001b[0m, in \u001b[0;36mrequire_status_code\u001b[0;34m(response, status_codes, get_status_code, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m         callback()\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mInvalidResponse(\n\u001b[1;32m    109\u001b[0m         response,\n\u001b[1;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRequest failed with status code\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         status_code,\n\u001b[1;32m    112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected one of\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         \u001b[39m*\u001b[39mstatus_codes\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m status_code\n",
      "\u001b[0;31mInvalidResponse\u001b[0m: ('Request failed with status code', 412, 'Expected one of', <HTTPStatus.OK: 200>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPreconditionFailed\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     json\u001b[39m.\u001b[39mdump(docs_dict, f)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Upload to blob storage\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m upload_blob(SCRAPED_TEXT_BUCKET, JSON_FILE_NAME, \u001b[39m'\u001b[39;49m\u001b[39mscraped_website_text.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m, in \u001b[0;36mupload_blob\u001b[0;34m(bucket_name, source_file_name, destination_blob_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Optional: set a generation-match precondition to avoid potential race conditions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# and data corruptions. The request to upload is aborted if the object's\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# generation number does not match your precondition. For a destination\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# object that does not yet exist, set the if_generation_match precondition to 0.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# If the destination object already exists in your bucket, set instead a\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# generation-match precondition using its generation number.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m generation_match_precondition \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 22\u001b[0m blob\u001b[39m.\u001b[39;49mupload_from_filename(source_file_name, if_generation_match\u001b[39m=\u001b[39;49mgeneration_match_precondition)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00msource_file_name\u001b[39m}\u001b[39;00m\u001b[39m uploaded to \u001b[39m\u001b[39m{\u001b[39;00mdestination_blob_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/cloud/storage/blob.py:2684\u001b[0m, in \u001b[0;36mBlob.upload_from_filename\u001b[0;34m(self, filename, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2682\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file_obj:\n\u001b[1;32m   2683\u001b[0m     total_bytes \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfstat(file_obj\u001b[39m.\u001b[39mfileno())\u001b[39m.\u001b[39mst_size\n\u001b[0;32m-> 2684\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupload_from_file(\n\u001b[1;32m   2685\u001b[0m         file_obj,\n\u001b[1;32m   2686\u001b[0m         content_type\u001b[39m=\u001b[39;49mcontent_type,\n\u001b[1;32m   2687\u001b[0m         num_retries\u001b[39m=\u001b[39;49mnum_retries,\n\u001b[1;32m   2688\u001b[0m         client\u001b[39m=\u001b[39;49mclient,\n\u001b[1;32m   2689\u001b[0m         size\u001b[39m=\u001b[39;49mtotal_bytes,\n\u001b[1;32m   2690\u001b[0m         predefined_acl\u001b[39m=\u001b[39;49mpredefined_acl,\n\u001b[1;32m   2691\u001b[0m         if_generation_match\u001b[39m=\u001b[39;49mif_generation_match,\n\u001b[1;32m   2692\u001b[0m         if_generation_not_match\u001b[39m=\u001b[39;49mif_generation_not_match,\n\u001b[1;32m   2693\u001b[0m         if_metageneration_match\u001b[39m=\u001b[39;49mif_metageneration_match,\n\u001b[1;32m   2694\u001b[0m         if_metageneration_not_match\u001b[39m=\u001b[39;49mif_metageneration_not_match,\n\u001b[1;32m   2695\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   2696\u001b[0m         checksum\u001b[39m=\u001b[39;49mchecksum,\n\u001b[1;32m   2697\u001b[0m         retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m   2698\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/cloud/storage/blob.py:2557\u001b[0m, in \u001b[0;36mBlob.upload_from_file\u001b[0;34m(self, file_obj, rewind, size, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_properties(created_json)\n\u001b[1;32m   2556\u001b[0m \u001b[39mexcept\u001b[39;00m resumable_media\u001b[39m.\u001b[39mInvalidResponse \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m-> 2557\u001b[0m     _raise_from_invalid_response(exc)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/dastex311/lib/python3.11/site-packages/google/cloud/storage/blob.py:4373\u001b[0m, in \u001b[0;36m_raise_from_invalid_response\u001b[0;34m(error)\u001b[0m\n\u001b[1;32m   4369\u001b[0m     error_message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(error)\n\u001b[1;32m   4371\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mmethod\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00merror_message\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 4373\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_http_status(response\u001b[39m.\u001b[39mstatus_code, message, response\u001b[39m=\u001b[39mresponse)\n",
      "\u001b[0;31mPreconditionFailed\u001b[0m: 412 POST https://storage.googleapis.com/upload/storage/v1/b/dastex-scraped-text/o?uploadType=multipart&ifGenerationMatch=0: {\n  \"error\": {\n    \"code\": 412,\n    \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n    \"errors\": [\n      {\n        \"message\": \"At least one of the pre-conditions you specified did not hold.\",\n        \"domain\": \"global\",\n        \"reason\": \"conditionNotMet\",\n        \"locationType\": \"header\",\n        \"location\": \"If-Match\"\n      }\n    ]\n  }\n}\n: ('Request failed with status code', 412, 'Expected one of', <HTTPStatus.OK: 200>)"
     ]
    }
   ],
   "source": [
    "# This is just for storage purposes\n",
    "docs_dict = [dict(doc) for doc in clean_docs]\n",
    "\n",
    "# First write file to disk\n",
    "with open(JSON_FILE_NAME, 'w') as f:\n",
    "    json.dump(docs_dict, f)\n",
    "\n",
    "# Upload to blob storage\n",
    "upload_blob(SCRAPED_TEXT_BUCKET, JSON_FILE_NAME, 'scraped_website_text.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../data/scraped_website_text_products.json uploaded to scraped_website_text_products.json.\n"
     ]
    }
   ],
   "source": [
    "# This is just for storage purposes\n",
    "p_docs_dict = [dict(doc) for doc in p_clean_docs]\n",
    "\n",
    "# First write file to disk\n",
    "with open('../data/scraped_website_text_products.json', 'w') as f:\n",
    "    json.dump(p_docs_dict, f)\n",
    "\n",
    "# Upload to blob storage\n",
    "upload_blob(SCRAPED_TEXT_BUCKET, '../data/scraped_website_text_products.json', 'scraped_website_text_products.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erfolgreich analysieren: https://www.dastex.de/produktportfolio/oberbekleidung/sonderloesungen/transporttaschen/\n"
     ]
    }
   ],
   "source": [
    "model = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "# Define your desired data structure.\n",
    "\n",
    "class Product(BaseModel):\n",
    "    product_name: Optional[str] = Field(description=\"the names of the product found on the website\")\n",
    "    product_description: Optional[str] = Field(description=\"the descriptions of the products found on the website\")\n",
    "    product_sizes: Optional[List[str]] = Field(description=\"product sizes available\")\n",
    "    product_materials: Optional[List[str]] = Field(description=\"materials the products are made from\")\n",
    "    product_colours: Optional[List[str]] = Field(description=\"available product colours\")\n",
    "\n",
    "class Webpage(BaseModel):\n",
    "    page_title: str = Field(description=\"website title\")\n",
    "    page_description: str = Field(description=\"website description\")\n",
    "    page_summary: str = Field(description=\"summary of the website content\")\n",
    "    page_products: List[Product] \n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Webpage)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Generate details of a product.\\n{format_instructions}\\nProduct description: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "struct_json = []\n",
    "errors = []\n",
    "\n",
    "one = [x for x in clean_docs if x.metadata['source'] == 'https://www.dastex.de/produktportfolio/oberbekleidung/sonderloesungen/transporttaschen/']\n",
    "\n",
    "for doc in one:\n",
    "    # And a query intented to prompt a language model to populate the data structure.\n",
    "    page_query = doc.page_content\n",
    "    try:\n",
    "        _input = prompt.format_prompt(query=page_query)\n",
    "\n",
    "        output = model(_input.to_string())\n",
    "\n",
    "        out = parser.parse(output)\n",
    "        struct_json.append(\n",
    "            {'source': doc.metadata['source'],\n",
    "             'webpage': out}\n",
    "        )\n",
    "        print('Erfolgreich analysieren: {}'.format(doc.metadata['source']))\n",
    "    except:\n",
    "        print('Parse failed: {}'.format(doc.metadata['source']))\n",
    "        errors.append({'source': doc.metadata['source']})\n",
    "\n",
    "\n",
    "for parsed in struct_json:\n",
    "    print('Titel der Seite: \\n{}\\n'.format(parsed['webpage'].page_title))\n",
    "    print('Beschreibung der Seite: \\n{}'.format(parsed['webpage'].page_description))\n",
    "    print('\\nErkannte Produkte:\\n')\n",
    "    for prod in parsed['webpage'].page_products:\n",
    "        print('Name des Produkts: {}'.format(prod.product_name))\n",
    "        print('Verfügbare Produktgrößen: {}'.format(prod.product_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Product(product_name='Transporttasche mit Reißverschluss', product_description='Transporttasche mit Reißverschluss', product_sizes=['35 cm x 25 cm B x H'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Transporttasche mit 3 Druckknöpfen', product_description='Transporttasche mit 3 Druckknöpfen', product_sizes=['35,5 cm x 40 cm B x H'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Hüftbeutel mit integriertem Gürtel', product_description='Hüftbeutel mit integriertem Gürtel', product_sizes=['14 cm x 20 cm Höhe'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Gürteltasche', product_description='Gürteltasche', product_sizes=['22 cm x 15 cm B x H'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Tasche für Mobiltelefon', product_description='Tasche für Mobiltelefon', product_sizes=['50 x 140 x 25 mm B x H x T'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Tasche für Mobiltelefon', product_description='Tasche für Mobiltelefon', product_sizes=['85 x 160 x 25 mm B x H x T'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Werkzeugtasche klein', product_description='Werkzeugtasche klein', product_sizes=['50 mm x 150 mm Höhe'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Werkzeugtasche groß mit Stiftlasche innen', product_description='Werkzeugtasche groß mit Stiftlasche innen', product_sizes=['20 cm x 22 cm B x H'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Werkzeugtasche aus Hypalon', product_description='Werkzeugtasche aus Hypalon', product_sizes=['18 x 18 cm B x H'], product_materials=['Hypalon'], product_colours=['schwarz']),\n",
       " Product(product_name='Tasche groß', product_description='Tasche groß', product_sizes=['44 x 26 x 22 mm B x H x T'], product_materials=['Hypalon'], product_colours=['schwarz'])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_json[123]['webpage'].page_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.dastex.de/produktportfolio/oberbekleidung/sonderloesungen/transporttaschen/'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_json[123]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "\n",
    "for x in struct_json:\n",
    "    d = getattr(x['webpage'], 'page_products')\n",
    "    for i in d:\n",
    "        products.append([getattr(i, 'product_name'), x['source'], getattr(i, 'product_sizes'), getattr(i, 'product_materials'), getattr(i, 'product_colours'), getattr(i, 'product_description')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame(products)\n",
    "prod_df.columns = ['product', 'link', 'sizes', 'materials', 'colours', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.to_csv('../data/scraped_json.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_links = pd.read_json('../data/associations_excel.json', orient='index').reset_index()\n",
    "excel_links.columns = ['product', 'link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df.merge(excel_links, on='link').to_csv('../data/merged_products.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, length_function=len, chunk_overlap=0, add_start_index = True)\n",
    "docs_spl_lis = text_splitter.split_documents(clean_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<vertexai.language_models._language_models._PreviewTextGenerationModel object at 0x142edf0d0>, model_name='text-bison@001', temperature=0.0, max_output_tokens=1024, top_p=0.8, top_k=40, stop=None, project=None, location='us-central1', credentials=None, request_parallelism=5, max_retries=6, tuned_model_name=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "\n",
    "# Embedding model\n",
    "embedding = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
    "\n",
    "# LLM Model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0.0,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(ebd: Embeddings, index_path: Path) -> Tuple[VectorStore, BaseModel]:\n",
    "    \"\"\"Load embedded data from its persisting path.\n",
    "\n",
    "    Args:\n",
    "        ebd (Embeddings): embedding model\n",
    "        index_path (Path): persisting path of the embedded data\n",
    "\n",
    "    Returns:\n",
    "        Tuple[VectorStore, BaseModel]: Embedded data and the underlying vector store (database).\n",
    "    \"\"\"\n",
    "    vectorstore = Chroma(embedding_function=ebd, persist_directory=str(index_path))\n",
    "    index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "\n",
    "    return index, vectorstore\n",
    "\n",
    "\n",
    "def create_index_old(ebd, json_fname):\n",
    "\n",
    "    with open(JSON_FILE_NAME, \"rt\") as f:\n",
    "        texts_lis = json.load(f)\n",
    "\n",
    "    docs_lis = [\n",
    "        Document(page_content=text[\"text\"], metadata={key: value for key, value in text.items() if key != 'text'}) for text in texts_lis\n",
    "    ]\n",
    "\n",
    "    file_path = os.getcwd()\n",
    "    index_path = os.path.join(file_path, \"embeddings_website\")\n",
    "\n",
    "    if os.path.isdir(index_path):\n",
    "        print(\"Load embedding\")\n",
    "        index, vectorstore = load_embedding(ebd, index_path)\n",
    "    else:\n",
    "        print(\"Create embedding\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "        docs_spl_lis = text_splitter.split_documents(docs_lis)\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            docs_spl_lis, embedding, persist_directory=str(index_path)\n",
    "        )\n",
    "        index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "        vectorstore.persist()\n",
    "\n",
    "    return index, vectorstore\n",
    "\n",
    "def create_index(ebd, docs, embedding_name=\"embeddings_website\"):\n",
    "\n",
    "    with open(JSON_FILE_NAME, \"rt\") as f:\n",
    "        texts_lis = json.load(f)\n",
    "\n",
    "    file_path = os.getcwd()\n",
    "    index_path = os.path.join(file_path, embedding_name)\n",
    "\n",
    "    if os.path.isdir(index_path):\n",
    "        print(\"Load embedding\")\n",
    "        index, vectorstore = load_embedding(ebd, index_path)\n",
    "    else:\n",
    "        print(\"Create embedding\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, length_function=len, chunk_overlap=0, add_start_index = True)\n",
    "        docs_spl_lis = text_splitter.split_documents(docs)\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            docs_spl_lis, embedding, persist_directory=str(index_path)\n",
    "        )\n",
    "        index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "        vectorstore.persist()\n",
    "\n",
    "    return index, vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create embedding\n"
     ]
    }
   ],
   "source": [
    "# Create the vectorstore for the whole scraped dataset\n",
    "index, vectorstore = create_index(embedding, clean_docs)\n",
    "upload_from_directory(EMBEDDINGS_BUCKET, 'embeddings_website', 'embeddings_website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorstore.get()['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorstore.get()['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create embedding\n"
     ]
    }
   ],
   "source": [
    "# Create the vectorstore just for the scraped product dataset\n",
    "p_index, p_vectorstore = create_index(embedding, p_clean_docs, \"embeddings_website_products\")\n",
    "upload_from_directory(EMBEDDINGS_BUCKET, 'embeddings_website_products', 'embeddings_website_products')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
